{
 "cells": [
  {
   "source": [
    "# Using TensorTrade for Making a Simple Trading Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Source: https://levelup.gitconnected.com/using-tensortrade-for-making-a-simple-trading-algorithm-6fad4d9bc79c\n",
    "\n",
    "In this tutorial, I’m going to show how to use Ray with TensorTrade (TT) in order to create a profitable algorithm on a predictable sine curve. You may be asking yourself, why use something so simple when the real world is much more difficult to predict? This is a very good question, and there is a simple answer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let’s define two instruments we want to have in our portfolio. We’ll use the U.S. dollar and a fake coin called TensorTrade Coin."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e56081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.oms.instruments import Instrument\n",
    "\n",
    "USD = Instrument(\"USD\", 2, \"U.S. Dollar\")\n",
    "TTC = Instrument(\"TTC\", 8, \"TensorTrade Coin\")"
   ]
  },
  {
   "source": [
    "## Define action scheme\n",
    "\n",
    "Ideally, we expect our agent to sell at the peaks and buy at the troughs. I’ll define actions that will allow us to perform this behavior. The ActionScheme I’ve built is extremely simple, relying on only two internal states: cash and asset.\n",
    "\n",
    "State Meaning Table\n",
    "\n",
    "The following is a transition diagram showing the states along with the actions that can be made: stay and move.\n",
    "\n",
    "Transition Diagram\n",
    "\n",
    "For example, if I start in cash and choose move, then I’ll transition to asset and receive P(t)-P(t-1) as a reward, where P(t) is the price of the asset at time step t. The rewards ascribed to these arrows will be described in further detail below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1447cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete\n",
    "\n",
    "from tensortrade.env.default.actions import TensorTradeActionScheme\n",
    "\n",
    "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
    "from tensortrade.core import Clock\n",
    "from tensortrade.oms.instruments import ExchangePair\n",
    "from tensortrade.oms.wallets import Portfolio\n",
    "from tensortrade.oms.orders import (\n",
    "    Order,\n",
    "    proportion_order,\n",
    "    TradeSide,\n",
    "    TradeType\n",
    ")\n",
    "\n",
    "\n",
    "class BSH(TensorTradeActionScheme):\n",
    "    ''' action\n",
    "            0: buy\n",
    "            1: sell\n",
    "    '''\n",
    "\n",
    "    registered_name = \"bsh\"\n",
    "\n",
    "    def __init__(self, cash: 'Wallet', asset: 'Wallet'):\n",
    "        super().__init__()\n",
    "        self.cash = cash\n",
    "        self.asset = asset\n",
    "\n",
    "        self.listeners = []\n",
    "        self.action = 0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return Discrete(2)\n",
    "\n",
    "    def attach(self, listener):\n",
    "        self.listeners += [listener]\n",
    "        return self\n",
    "\n",
    "    def get_orders(self, action: int, portfolio: 'Portfolio'):\n",
    "        order = None\n",
    "\n",
    "        if abs(action - self.action) > 0:\n",
    "            src = self.cash if self.action == 0 else self.asset\n",
    "            tgt = self.asset if self.action == 0 else self.cash\n",
    "            order = proportion_order(portfolio, src, tgt, 1.0)\n",
    "            self.action = action\n",
    "\n",
    "        for listener in self.listeners:\n",
    "            listener.on_action(action)\n",
    "\n",
    "        return [order]\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.action = 0"
   ]
  },
  {
   "source": [
    "## Define reward scheme\n",
    "\n",
    "Next, I’ll create a reward scheme to reflect how well we are positioned in the environment. Essentially, we want a mapping that reflects the correct reward for each state we are in."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309158d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "from tensortrade.feed.core import Stream, DataFeed\n",
    "\n",
    "\n",
    "class PBR(TensorTradeRewardScheme):\n",
    "\n",
    "    registered_name = \"pbr\"\n",
    "\n",
    "    def __init__(self, price: 'Stream'):\n",
    "        super().__init__()\n",
    "        self.position = -1\n",
    "\n",
    "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
    "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
    "\n",
    "        reward = (r * position).fillna(0).rename(\"reward\")\n",
    "\n",
    "        self.feed = DataFeed([reward])\n",
    "        self.feed.compile()\n",
    "\n",
    "    def on_action(self, action: int):\n",
    "        self.position = -1 if action == 0 else 1\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio'):\n",
    "        return self.feed.next()[\"reward\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = -1\n",
    "        self.feed.reset()"
   ]
  },
  {
   "source": [
    "## Define renderer\n",
    "\n",
    "Finally, we would like to make sure we can see if the agent is selling at the peaks and buying at the troughs. Here is a Renderer that can display this information using Matplotlib."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "\n",
    "class PositionChangeChart(Renderer):\n",
    "\n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "        \n",
    "#         env.action_scheme.portfolio.performance.plot(ax=axs[1])\n",
    "        performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        columns = [1,2,4,5,7]\n",
    "        performance.drop(performance.columns[columns],axis=1,inplace=True)\n",
    "        performance.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "source": [
    "## Train the agent\n",
    "\n",
    "Frist, in order to use our custom environment in ray, we must write a function that creates an instance of the TradingEnv from a configuration dictionary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "y = 50*np.sin(3*x) + 100\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28018402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "def create_env(config):\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "    y = 50*np.sin(3*x) + 100\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.2\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)"
   ]
  },
  {
   "source": [
    "Next, since the environment is registered, we can use the Proximal Policy Optimization (PPO) algorithm from rllib to train our agent.\n",
    "\n",
    "To check if everything works, set a low episode_reward_mean so a fast check can be done (set at 10). Once everything is working, set it to 3000 and run the training. Once the training is complete, we can get access to the agent’s policy by restoring from the last checkpoint."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "      \"episode_reward_mean\": 500\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"INFO\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 2,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"INFO\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 2,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa921a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symfit import parameters, variables, sin, cos, Fit\n",
    "\n",
    "def fourier_series(x, f, n=0):\n",
    "    \"\"\"Creates a symbolic fourier series of order `n`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : `symfit.Variable`\n",
    "        The input variable for the function.\n",
    "    f : `symfit.Parameter`\n",
    "        Frequency of the fourier series\n",
    "    n : int\n",
    "        Order of the fourier series.\n",
    "    \"\"\"\n",
    "    # Make the parameter objects for all the terms\n",
    "    a0, *cos_a = parameters(','.join(['a{}'.format(i) for i in range(0, n + 1)]))\n",
    "    sin_b = parameters(','.join(['b{}'.format(i) for i in range(1, n + 1)]))\n",
    "\n",
    "    # Construct the series\n",
    "    series = a0 + sum(ai * cos(i * f * x) + bi * sin(i * f * x)\n",
    "                     for i, (ai, bi) in enumerate(zip(cos_a, sin_b), start=1))\n",
    "    return series\n",
    "\n",
    "\n",
    "def gbm(price: float,\n",
    "        mu: float,\n",
    "        sigma: float,\n",
    "        dt: float,\n",
    "        n: int) -> np.array:\n",
    "    \"\"\"Generates a geometric brownian motion path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    price : float\n",
    "        The initial price of the series.\n",
    "    mu : float\n",
    "        The percentage drift.\n",
    "    sigma : float\n",
    "        The percentage volatility.\n",
    "    dt : float\n",
    "        The time step size.\n",
    "    n : int\n",
    "        The number of steps to be generated in the path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `np.array`\n",
    "        The generated path.\n",
    "    \"\"\"\n",
    "    y = np.exp((mu - sigma ** 2 / 2) * dt + sigma * np.random.normal(0, np.sqrt(dt), size=n).T)\n",
    "    y = price * y.cumprod(axis=0)\n",
    "    return y\n",
    "\n",
    "\n",
    "def fourier_gbm(price, mu, sigma, dt, n, order):\n",
    "\n",
    "    x, y = variables('x, y')\n",
    "    w, = parameters('w')\n",
    "    model_dict = {y: fourier_series(x, f=w, n=order)}\n",
    "\n",
    "    # Make step function data\n",
    "    xdata = np.arange(-np.pi, np.pi, 2*np.pi / n)\n",
    "    ydata = np.log(gbm(price, mu, sigma, dt, n))\n",
    "\n",
    "    # Define a Fit object for this model and data\n",
    "    fit = Fit(model_dict, x=xdata, y=ydata)\n",
    "    fit_result = fit.execute()\n",
    "\n",
    "    return np.exp(fit.model(x=xdata, **fit_result.params).y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d47382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_env(config):\n",
    "    y = config[\"y\"]\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.2\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "for _ in range(6):\n",
    "    # Instantiate the environment\n",
    "    env = create_eval_env({\n",
    "        \"window_size\": 25,\n",
    "        \"y\": fourier_gbm(price=100, mu=0.1, sigma=0.5, dt=0.01, n=1000, order=5)\n",
    "    })\n",
    "\n",
    "\n",
    "    # Run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    env.render()"
   ]
  },
  {
   "source": [
    "As you can see, the agent has been able to make correct decisions on some of the price curves, but not all of them. In particular, the last three curves showcase some of the shortcomings of the agent. In samples 4 and 5, it doesn’t seem to understand that the general trend is downward, and that it should transfer all of its assets into cash. For sample 6, the agent seemed to stop making decisions altogether. This can most likely be attributed to the high volatility of the price curve, containing many local minima and maxima relative to the first three curves. With the price changing so fast, the agent decides that the best option is to hold the entire time.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Test trained model with cryptocurrency data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.data.cdd import CryptoDataDownload\n",
    "from tensortrade.oms.instruments import USD, BTC, ETH\n",
    "\n",
    "def create_env(config, data):\n",
    "    p = Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"USD-BTC\")\n",
    "    bitstamp = Exchange(\"bitstamp\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitstamp, 10000 * USD)\n",
    "    asset = Wallet(bitstamp, 0.05 * BTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(data[\"close\"], dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.2\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.data.cdd import CryptoDataDownload\n",
    "from tensortrade.oms.instruments import USD, BTC, ETH\n",
    "\n",
    "cdd = CryptoDataDownload()\n",
    "\n",
    "data = cdd.fetch(\"Bitstamp\", \"USD\", \"BTC\", \"1h\")\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "# df.describe()\n",
    "\n",
    "plt.plot(df['date'], df['close'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = create_env({\"window_size\": 25}, df)\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(len(df.index)):\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if _ % 1000 == 0: print(info) \n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69157cf-81b0-4a1e-a84f-b279b168c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasgui import show\n",
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "performance.to_csv('result/BTC_USD_Sample_Model_Performance_{}.csv'.format(today))\n",
    "gui = show(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45550f-4ba4-4abc-ba0a-ad61f2bce80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger = env.action_scheme.portfolio.ledger.as_frame()\n",
    "gui = show(ledger)"
   ]
  },
  {
   "source": [
    "## Conclusion\n",
    "Our main goal was to test that our reward and action scheme could be used to train an agent to make profitable decisions on a simple sine curve. Despite the suboptimal performance on some of the latter samples, the objective here was to get insight into agent decisions and inform how we engineer future reward schemes. From these basic concepts, you can work up to building more complex action and reward schemes, from which successful algorithms can be made. There is no limit to the kinds of trading environments that can be created with TensorTrade, and I encourage everybody to experiment with the different possibilities. So that about wraps up this tutorial! You can checkout the TensorTrade library to learn more on how to use it to make your own trading environments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0f7dcdf823f933b5f1d2e52d799331ca40ebe48ced06a01d002860e26a42ab79a",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}